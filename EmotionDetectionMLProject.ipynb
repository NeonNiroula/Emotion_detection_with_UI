{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnZe1KfsNROr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFvU9pIgEi51"
      },
      "outputs": [],
      "source": [
        "!pip freeze > requirnments.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqQv54zdf3s_",
        "outputId": "b817d948-75f1-4c52-d18a-679fcc824564"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbi1i6yQNUT0"
      },
      "outputs": [],
      "source": [
        "# Load text data\n",
        "text_data = pd.read_csv('/content/drive/MyDrive/EmotionDetection/iemocap_full_dataset.csv')\n",
        "\n",
        "# Function to preprocess image\n",
        "def preprocess_image(image):\n",
        "    # Resize image to a fixed size (e.g., 48x48 pixels)\n",
        "    resized_image = cv2.resize(image, (48, 48))\n",
        "    # Convert image to grayscale\n",
        "    grayscale_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
        "    # Normalize pixel values to [0, 1]\n",
        "    normalized_image = grayscale_image / 255.0\n",
        "    return normalized_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rR5AS4xPNmf4"
      },
      "outputs": [],
      "source": [
        "# Define the directory containing CK+ images\n",
        "ck_plus_images_dir = '/content/drive/MyDrive/EmotionDetection/CK+'\n",
        "\n",
        "# Initialize lists to store images and labels\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "# Define a mapping of emotion labels to numerical values\n",
        "emotion_label_map = {\n",
        "    'anger': 0,\n",
        "    'contempt': 1,\n",
        "    'disgust': 2,\n",
        "    'fear': 3,\n",
        "    'happy': 4,\n",
        "    'sadness': 5,\n",
        "    'surprise': 6\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9br1wdAN3u2"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty list to store images\n",
        "images = []\n",
        "\n",
        "labels_list = []\n",
        "\n",
        "# Loop through each directory (corresponding to each emotion label)\n",
        "for emotion_dir in os.listdir(ck_plus_images_dir):\n",
        "    # Skip non-directory files\n",
        "    if not os.path.isdir(os.path.join(ck_plus_images_dir, emotion_dir)):\n",
        "        continue\n",
        "\n",
        "    # Map the emotion label to its numerical value\n",
        "    label = emotion_label_map.get(emotion_dir.lower())  # Use lower() to ensure case-insensitive mapping\n",
        "    if label is None:\n",
        "        continue  # Skip if the emotion label is not defined in the mapping\n",
        "\n",
        "    # Loop through each image file in the directory\n",
        "    for filename in os.listdir(os.path.join(ck_plus_images_dir, emotion_dir)):\n",
        "        if filename.endswith('.png'):  # Assuming images are in PNG format\n",
        "            # Read the image\n",
        "            image = cv2.imread(os.path.join(ck_plus_images_dir, emotion_dir, filename))\n",
        "            if image is None:\n",
        "                continue  # Skip if image loading fails\n",
        "            # Preprocess the image\n",
        "            preprocessed_image = preprocess_image(image)\n",
        "            # Append the preprocessed image to the list of images\n",
        "            images.append(preprocessed_image)\n",
        "            # Append the label to the list of labels\n",
        "            labels_list.append(label)\n",
        "\n",
        "# Convert the list of labels to a NumPy array\n",
        "labels = np.array(labels_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRc9kCZNPqCG"
      },
      "outputs": [],
      "source": [
        "# Convert lists to NumPy arrays\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNS4DKiLVGdh",
        "outputId": "cb6de6df-35e6-4468-ccb9-6c1e7afc16ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique Labels: [0 1 2 3 4 5 6]\n",
            "Label Counts: [135  54 177  75 207  84 249]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Check the label distribution\n",
        "unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
        "print(\"Unique Labels:\", unique_labels)\n",
        "print(\"Label Counts:\", label_counts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "fWKi0Tz_Praz",
        "outputId": "44afcae5-0a9a-464d-bfce-6f837afa655c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "RandomForestClassifier()"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Initialize and train a Support Vector Machine (SVM) classifier\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train.reshape(len(X_train), -1), y_train)\n",
        "\n",
        "# Initialize and train a Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
        "rf_classifier.fit(X_train.reshape(len(X_train), -1), y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zefR75EoPwEB",
        "outputId": "26f3acf1-b47b-4f8e-8a51-f26af6d6701b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=3)\n",
        "clusters = kmeans.fit_predict(X_train.reshape(len(X_train), -1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1ZjL8-RP0A-"
      },
      "outputs": [],
      "source": [
        "# Define the CNN model for facial expression analysis\n",
        "def build_cnn_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(7, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBqIh7saP32N",
        "outputId": "81ca0cea-2b70-4cb1-9d4c-2764d48ac242"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "25/25 [==============================] - 4s 17ms/step - loss: 1.7774 - accuracy: 0.2959\n",
            "Epoch 2/10\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 1.4080 - accuracy: 0.5319\n",
            "Epoch 3/10\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.9368 - accuracy: 0.6837\n",
            "Epoch 4/10\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.5899 - accuracy: 0.8061\n",
            "Epoch 5/10\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.3465 - accuracy: 0.9069\n",
            "Epoch 6/10\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.2132 - accuracy: 0.9349\n",
            "Epoch 7/10\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.1610 - accuracy: 0.9592\n",
            "Epoch 8/10\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.1088 - accuracy: 0.9732\n",
            "Epoch 9/10\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.0880 - accuracy: 0.9796\n",
            "Epoch 10/10\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.0639 - accuracy: 0.9872\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x798c28cbbe20>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Build the CNN model\n",
        "cnn_model = build_cnn_model()\n",
        "cnn_model.fit(X_train.reshape(-1, 48, 48, 1), y_train, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RUmj8etSYY3",
        "outputId": "6fd8990f-1986-4dc7-a6e5-a5d47c4b3a3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM Accuracy: 0.9949238578680203\n",
            "Random Forest Accuracy: 0.9847715736040609\n",
            "7/7 [==============================] - 0s 20ms/step - loss: 0.0835 - accuracy: 0.9746\n",
            "CNN Accuracy: 0.9746192693710327\n"
          ]
        }
      ],
      "source": [
        "# Evaluate SVM model\n",
        "svm_accuracy = svm_classifier.score(X_test.reshape(len(X_test), -1), y_test)\n",
        "\n",
        "# Evaluate Random Forest model\n",
        "rf_accuracy = rf_classifier.score(X_test.reshape(len(X_test), -1), y_test)\n",
        "\n",
        "print(\"SVM Accuracy:\", svm_accuracy)\n",
        "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
        "\n",
        "# Evaluate CNN model\n",
        "cnn_loss, cnn_accuracy = cnn_model.evaluate(X_test.reshape(-1, 48, 48, 1), y_test)\n",
        "\n",
        "print(\"CNN Accuracy:\", cnn_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ha0FHs6oQPLG"
      },
      "outputs": [],
      "source": [
        "# Save the pre-trained CNN model\n",
        "cnn_model.save('emotion_recognition_model.keras')\n",
        "\n",
        "# Load the pretrained CNN model for emotion detection\n",
        "cnn_model = tf.keras.models.load_model('emotion_recognition_model.keras')\n",
        "\n",
        "# Define the emotion labels\n",
        "emotion_labels = {0: 'Anger', 1: 'Contempt', 2: 'Disgust', 3: 'Fear', 4: 'Happy', 5: 'Sadness', 6: 'Surprise'}\n",
        "\n",
        "# Function to detect emotion from an image frame\n",
        "def detect_emotion(frame):\n",
        "    # Preprocess the frame\n",
        "    resized_frame = cv2.resize(frame, (48, 48))\n",
        "    grayscale_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2GRAY)\n",
        "    normalized_frame = grayscale_frame / 255.0\n",
        "    input_frame = np.expand_dims(normalized_frame, axis=-1)\n",
        "    input_frame = np.expand_dims(input_frame, axis=0)  # Add batch dimension\n",
        "\n",
        "     # Perform emotion prediction\n",
        "    predictions = cnn_model.predict(input_frame)\n",
        "    predicted_class = np.argmax(predictions)\n",
        "    emotion_label = emotion_labels[predicted_class]\n",
        "\n",
        "    return emotion_label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYvwRr8JQq9n",
        "outputId": "00674c26-57d4-4901-b886-1197f79dce89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully!\n",
            "ok\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Assuming the model is saved as 'emotion_recognition_model.keras' in your Drive\n",
        "cnn_model = tf.keras.models.load_model('/content/emotion_recognition_model.keras')\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "\n",
        "# Define the emotion labels\n",
        "emotion_labels = {0: 'Anger', 1: 'Contempt', 2: 'Disgust', 3: 'Fear', 4: 'Happy', 5: 'Sadness', 6: 'Surprise'}\n",
        "\n",
        "image_path = \"/content/drive/MyDrive/EmotionDetection/S029_001_00000019.png\"\n",
        "image = cv2.imread(image_path)\n",
        "print('ok')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBovEqAPov1D"
      },
      "outputs": [],
      "source": [
        "# Preprocess the image\n",
        "def preprocess_image(image):\n",
        "  # Resize image to a fixed size (e.g., 48x48 pixels) following the model's input shape\n",
        "  resized_image = cv2.resize(image, (48, 48))\n",
        "  # Convert image to grayscale\n",
        "  grayscale_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
        "  # Normalize pixel values to [0, 1]\n",
        "  normalized_image = grayscale_image / 255.0\n",
        "  return normalized_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smEFocBxoxyJ",
        "outputId": "ba6e8a37-08ed-4849-9bd6-5b577c3df82a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 264ms/step\n",
            "Predicted Emotion: Anger\n"
          ]
        }
      ],
      "source": [
        "# Preprocess the image using the defined function\n",
        "preprocessed_image = preprocess_image(image)\n",
        "\n",
        "# Add a batch dimension for the CNN model\n",
        "input_frame = np.expand_dims(preprocessed_image, axis=-1)\n",
        "input_frame = np.expand_dims(input_frame, axis=0)\n",
        "\n",
        "# Now you have the preprocessed image (`input_frame`) ready for prediction\n",
        "# using the loaded CNN model (`cnn_model`)\n",
        "\n",
        "# Example usage (assuming you have a loaded `cnn_model`)\n",
        "predictions = cnn_model.predict(input_frame)\n",
        "predicted_class = np.argmax(predictions)\n",
        "emotion_label = emotion_labels[predicted_class]\n",
        "\n",
        "print(\"Predicted Emotion:\", emotion_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFCMeMPQJm50",
        "outputId": "65f209d4-8613-4216-a8cc-af7a1727d742"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install streamlit -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a-jaEykJsXN",
        "outputId": "4ba2904f-4b57-45ec-fbd1-d36ff5d2eed1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the pre-trained CNN model for emotion detection\n",
        "cnn_model = tf.keras.models.load_model('emotion_recognition_model.keras')\n",
        "\n",
        "# Define the emotion labels\n",
        "emotion_labels = {0: 'Anger', 1: 'Contempt', 2: 'Disgust', 3: 'Fear', 4: 'Happy', 5: 'Sadness', 6: 'Surprise'}\n",
        "\n",
        "# Function to preprocess image\n",
        "def preprocess_image(image):\n",
        "    resized_image = cv2.resize(image, (48, 48))\n",
        "    grayscale_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
        "    normalized_image = grayscale_image / 255.0\n",
        "    return normalized_image\n",
        "\n",
        "# Function to detect emotion from an image frame\n",
        "def detect_emotion(frame):\n",
        "    resized_frame = cv2.resize(frame, (48, 48))\n",
        "    grayscale_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2GRAY)\n",
        "    normalized_frame = grayscale_frame / 255.0\n",
        "    input_frame = np.expand_dims(normalized_frame, axis=-1)\n",
        "    input_frame = np.expand_dims(input_frame, axis=0)  # Add batch dimension\n",
        "\n",
        "    predictions = cnn_model.predict(input_frame)\n",
        "    predicted_class = np.argmax(predictions)\n",
        "    emotion_label = emotion_labels[predicted_class]\n",
        "\n",
        "    return emotion_label\n",
        "\n",
        "# Streamlit app\n",
        "def main():\n",
        "    st.title(\"Emotion Detection App\")\n",
        "\n",
        "    uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"png\"])\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        try:\n",
        "            # Read the image\n",
        "            image = cv2.imdecode(np.fromstring(uploaded_file.read(), np.uint8), 1)\n",
        "\n",
        "            # Display the uploaded image\n",
        "            st.image(image, caption='Uploaded Image', use_column_width=True)\n",
        "\n",
        "            # Detect emotion upon button press\n",
        "            if st.button('Predict Emotion'):\n",
        "                # Preprocess the image\n",
        "                preprocessed_image = preprocess_image(image)\n",
        "\n",
        "                # Add a batch dimension\n",
        "                input_frame = np.expand_dims(preprocessed_image, axis=-1)\n",
        "                input_frame = np.expand_dims(input_frame, axis=0)\n",
        "\n",
        "                # Predict emotion\n",
        "                predicted_emotion = detect_emotion(image)\n",
        "\n",
        "                # Display predicted emotion\n",
        "                st.success(f\"Predicted Emotion: {predicted_emotion}\")\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQeUWGeRYujv",
        "outputId": "eba75d40-e9dc-462e-f482-6397bf9bc78d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.139.9.79\n"
          ]
        }
      ],
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvFKArQ4ZPjg",
        "outputId": "6384e50e-3cc4-45d3-eb00-7ebe797a5d5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: streamlit: command not found\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 3.765s\n",
            "your url is: https://thick-clubs-wash.loca.lt\n"
          ]
        }
      ],
      "source": [
        "! streamlit run app.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}